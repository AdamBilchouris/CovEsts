% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kernel_regression_estimator.R
\name{compute_truncated_est}
\alias{compute_truncated_est}
\title{Compute the Truncated Kernel Regression Estimator.}
\usage{
compute_truncated_est(
  X,
  x,
  t,
  T1,
  T2,
  b,
  kernel_name = "gaussian",
  kernel_params = c(),
  custom_kernel = FALSE,
  pd = TRUE,
  type = "autocovariance",
  meanX = mean(X)
)
}
\arguments{
\item{X}{A vector representing the observed values of the process.}

\item{x}{A vector of lags.}

\item{t}{The arguments at which the autocovariance function is calculated at.}

\item{T1}{The first truncation point, \eqn{T_{1} > 0.}}

\item{T2}{The second truncation point, \eqn{T_{2} > T_{1} > 0.}}

\item{b}{Bandwidth parameter, greater than 0.}

\item{kernel_name}{The name of the symmetric kernel (see \link{kernel_symm}) function to be used. Possible values are:
"gaussian", "wave", "rational_quadratic", and "bessel_j". Alternatively, a custom kernel function can be provided, see the examples.}

\item{kernel_params}{A vector of parameters of the kernel function. See \link{kernel_symm} for parameters.}

\item{custom_kernel}{If a custom kernel is to be used or not. Defaults to \code{FALSE}.}

\item{pd}{Whether a positive-definite estimate should be used. Defaults to \code{TRUE}.}

\item{type}{Compute either the 'autocovariance' or 'autocorrelation'. Defaults to 'autocovariance'.}

\item{meanX}{The average value of \code{X}. Defaults to \code{mean(X)}.}
}
\value{
A vector whose values are the truncated kernel regression estimates.
}
\description{
This function computes the truncated kernel regression estimator,
\deqn{\hat{\rho}_{1}(t)  = \left\{ \begin{array}{ll}
\hat{\rho}(t) & 0 \leq t \leq T_{1} \\
\hat{\rho}(T_{1}) (T_{2} - t)(T_{2} - T_{1})^{-1} & T_{1} < t \leq T_{2} \\
0 & t > T_{2}
\end{array} , \right. }
where \eqn{\hat{\rho}(\cdot)} is the kernel regression estimator, see \link{compute_adjusted_est}.
}
\details{
This function computes the truncated kernel regression estimator,
\deqn{\hat{\rho}_{1}(t) = \left\{ \begin{array}{ll}
\hat{\rho}(t) & 0 \leq t \leq T_{1} \\
\hat{\rho}(T_{1}) (T_{2} - t)(T_{2} - T_{1})^{-1} & T_{1} < t \leq T_{2} \\
0 & t > T_{2}
\end{array} \right. }
where \eqn{\hat{\rho}(\cdot)} is the kernel regression estimator, see \link{compute_adjusted_est}.

Compared to \link{compute_adjusted_est}, this function brings down the estimate to zero linearly between \eqn{T_{1}} and \eqn{T_{2}}.
In the case of short-range dependence, this may be beneficial as it can remove estimation artefacts at large lags.

To make this estimator positive-definite, the following procedure is used:
\enumerate{
\item Take the discrete cosine transform
\eqn{\mathcal{F}^{c}(\hat{\rho}_{1}(t))}.
\item Find the smallest frequency where its associated value in the spectral domain is negative
\deqn{\hat{\theta} = \inf\{ \theta > 0 :  \mathcal{F}^{c}(\hat{\rho}_{1}(t)) < 0\}.}
\item Set all values starting at the frequency to zero.
\item Perform the inversion.
}

This ensures the autocovariance function estimate is positive-definite. If \eqn{\hat{\theta}} is the first nonzero sample frequency,
the entire spectrum, apart from an impulse at zero, is zero, resulting in an adjusted function that is a horizontal line whose value is the area of the of the estimated function prior to the adjustment.
}
\examples{
X <- c(1, 2, 3, 4)
compute_truncated_est(X, 1:4, 1:3, 1, 2, 0.1,
                  "gaussian", c(), FALSE, TRUE, meanX = mean(X))

my_kernel <- function(x, theta, params) {
  stopifnot(theta > 0, length(x) >= 1)
  return(exp(-((abs(x) / theta)^params[1])) * (2 * theta  * gamma(1 + 1/params[1])))
}
compute_truncated_est(X, 1:4, 1:3, 1, 2, 0.1, "my_kernel", c(0.25), TRUE, TRUE)
}
\references{
Hall, P., & Patil, P. (1994). Properties of nonparametric estimators of autocovariance for stationary random fields. Probability Theory and Related Fields (Vol. 99, Issue 3, pp. 399–424). 10.1007/bf01199899

Hall, P., Fisher, N. I., & Hoffmann, B. (1994). On the nonparametric estimation of covariance functions. The Annals of Statistics (Vol. 22, Issue 4, pp. 2115–2134). 10.1214/aos/1176325774
}
